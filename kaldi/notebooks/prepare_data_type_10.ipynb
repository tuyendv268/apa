{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/prep_gopt/egs/librispeech/s5/\n",
    "from pandarallel import pandarallel\n",
    "from asr import Whisper_STT\n",
    "from glob import glob\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "pandarallel.initialize(nb_workers=5, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = \"/data/audio_data/prep_submission_audio/10\"\n",
    "metadata_path=\"/data/audio_data/pronunciation_scoring_result/info_question_type-10_01082022_18092023.csv\"\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio_is_exist(audio_id):\n",
    "    abs_path = os.path.join(audio_dir, f'{audio_id}.wav')\n",
    "    if os.path.exists(abs_path):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "metadata[\"is_exist\"] =  metadata.id.parallel_apply(check_audio_is_exist)\n",
    "print(metadata.shape)\n",
    "metadata = metadata[metadata[\"is_exist\"] == True]\n",
    "metadata.reset_index(inplace=True)\n",
    "print(metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = re.sub('[\\!@#$%^&*\\(\\)\\\\\\.\\'\\\"\\,\\?\\;\\:\\+\\-\\_\\/\\|~`]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.upper().strip()\n",
    "    return text\n",
    "\n",
    "def load_lexicon(path=\"resources/lexicon.txt\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.readlines()\n",
    "        lines = [line.strip() for line in content]\n",
    "    lexicon = {}\n",
    "    for line in lines:\n",
    "        tmp = line.split()\n",
    "        word, arpabet = tmp[0], \" \".join(tmp[1:])\n",
    "\n",
    "        if word not in lexicon:\n",
    "            lexicon[word] = [arpabet, ]\n",
    "        else:\n",
    "            lexicon[word].append(arpabet)\n",
    "\n",
    "    for key in lexicon.keys():\n",
    "        lexicon[key] = set(lexicon[key])\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "lexicon = load_lexicon(path=\"/data/codes/prep_gopt/egs/librispeech/s5/data/lexicon.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_word(word):\n",
    "    text = normalize(word[\"text\"])\n",
    "\n",
    "    if text not in lexicon:\n",
    "        return False\n",
    "    else:\n",
    "        if word[\"trans_arpabet\"] not in lexicon[text]:\n",
    "            return False      \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    words, text = [], []\n",
    "    start_time, end_time = sentence[0][\"start_time\"], sentence[-1][\"end_time\"]\n",
    "    for word in sentence:\n",
    "        if not is_valid_word(word):\n",
    "            return None\n",
    "\n",
    "        text.append(word[\"text\"])\n",
    "        words.append(\n",
    "            {\n",
    "                'text': word[\"text\"],\n",
    "                'arpabet': word[\"trans_arpabet\"],\n",
    "                'start_time': word[\"start_time\"],\n",
    "                'end_time': word[\"end_time\"],\n",
    "                'score': word[\"nativeness_score\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "    sentence = {\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"text\": \" \".join(text),\n",
    "            \"words\": words,\n",
    "        }\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(json_path):\n",
    "    try:\n",
    "        with open(json_path, \"r\") as f:\n",
    "            raw_sample = json.load(f)\n",
    "        \n",
    "        sentences = []\n",
    "\n",
    "        assert len(raw_sample[\"utterance\"]) == 1\n",
    "        sample_id = os.path.basename(json_path).split(\".\")[0]\n",
    "        for index, utterance in enumerate(raw_sample[\"utterance\"]):\n",
    "            parsed_sent = parse_sentence(utterance[\"words\"].copy())\n",
    "            if parsed_sent is None:\n",
    "                continue\n",
    "\n",
    "            parsed_sent[\"utt_id\"] = f'{sample_id}'\n",
    "            parsed_sent[\"id\"] = f'{sample_id}'\n",
    "            sentences.append(parsed_sent)\n",
    "\n",
    "        return sentences\n",
    "    \n",
    "    except:\n",
    "        return []\n",
    "\n",
    "json_dir = \"/data/audio_data/pronunciation_scoring_result/marking_data/10\"\n",
    "tmp = metadata.id.parallel_apply(lambda x: preprocess_data(os.path.join(json_dir, f'{x}.json')))\n",
    "# tmp = metadata.head(10).id.apply(lambda x: preprocess_data(os.path.join(json_dir, f'{x}.json')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = tmp.apply(lambda x: len(x))\n",
    "tmp[count!=0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = tmp.apply(lambda x: len(x))\n",
    "tmp[count!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0\n",
    "for index in tmp.index:\n",
    "    for sent in tmp.iloc[index]:\n",
    "        total_time += (sent[\"end_time\"] - sent[\"start_time\"])\n",
    "\n",
    "total_time / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_metadata = tmp.explode()\n",
    "\n",
    "df = pd.DataFrame(tmp_metadata.values, columns=['sent'])\n",
    "df.dropna(inplace=True)\n",
    "df[\"sent\"] = df[\"sent\"].apply(lambda x: json.dumps(x, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/data/codes/prep_gopt/egs/librispeech/s5/data/stt/raw/metadata_type_10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm \n",
    "\n",
    "# question_id = tmp_metadata.question_id.value_counts()\n",
    "# filtered_datas = []\n",
    "# for index in tqdm(question_id.index):\n",
    "#     if tmp_metadata[tmp_metadata.question_id == index].shape[0] > 1000:\n",
    "#         tmp = tmp_metadata[tmp_metadata.question_id == index][0:1000]\n",
    "#     else:\n",
    "#         tmp = tmp_metadata[tmp_metadata.question_id == index]\n",
    "#     filtered_datas.append(tmp.sample(frac=1)[0:1000])\n",
    "\n",
    "# filtered_metadata = pd.concat(filtered_datas)\n",
    "# filtered_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# in_dir = '/data/audio_data/prep_submission_audio/10'\n",
    "# out_dir = '/data/codes/prep_gopt/egs/librispeech/s5/data/prep/wav'\n",
    "# def get_audio(sent):\n",
    "#     sent = json.loads(sent)\n",
    "#     utt_id = sent[\"utt_id\"]\n",
    "\n",
    "#     in_path = f'{in_dir}/{sent[\"id\"]}.wav'\n",
    "#     out_path = f'{out_dir}/{utt_id}.wav'\n",
    "\n",
    "#     shutil.copyfile(in_path, out_path)\n",
    "\n",
    "# df.sent.parallel_apply(get_audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
