{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "%cd /data/codes/apa/train/\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "from src.dataset import PrepDataset\n",
    "from src.model import PrepModel\n",
    "from src.utils.train import (\n",
    "    load_data,\n",
    "    convert_score_to_color,\n",
    "    to_device,\n",
    "    valid_phn,\n",
    "    valid_utt,\n",
    "    valid_wrd,\n",
    "    to_cpu,\n",
    "    load_pred_and_label,\n",
    "    save_confusion_matrix_figure,\n",
    "    validate,\n",
    "    save\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = '/data/codes/apa/train/exp/ckpts/dev'\n",
    "in_dir = \"/data/codes/apa/train/data/feats/train/train-data-type-12\"\n",
    "out_dir = f'{in_dir}-filtered'\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128\n",
    "relative2id_path=\"/data/codes/apa/train/exp/dicts/relative2id.json\"\n",
    "phone2id_path=\"/data/codes/apa/train/exp/dicts/phone_dict.json\"\n",
    "\n",
    "ids, phone_ids_path, word_ids_path, \\\n",
    "    phone_scores_path, word_scores_path, sentence_scores_path, fluency_score_path, intonation_score_path, \\\n",
    "    durations_path, gops_path, relative_positions_path, wavlm_features_path = load_data(in_dir)\n",
    "\n",
    "dataset = PrepDataset(\n",
    "    ids=ids, \n",
    "    phone_ids_path=phone_ids_path, \n",
    "    word_ids_path=word_ids_path, \n",
    "    phone_scores_path=phone_scores_path, \n",
    "    word_scores_path=word_scores_path, \n",
    "    sentence_scores_path=sentence_scores_path, \n",
    "    fluency_score_path=fluency_score_path,\n",
    "    intonation_scores_path=intonation_score_path,\n",
    "    durations_path=durations_path, \n",
    "    gops_path=gops_path, \n",
    "    relative_positions_path=relative_positions_path, \n",
    "    wavlm_features_path=wavlm_features_path,\n",
    "    relative2id_path=relative2id_path, \n",
    "    phone2id_path=phone2id_path,\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=8, \n",
    "    num_workers=1,\n",
    "    shuffle=True, \n",
    "    drop_last=True, \n",
    "    pin_memory=True, \n",
    ")\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    batch = dataset[i]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim=32\n",
    "num_heads=1\n",
    "depth=3\n",
    "input_dim=855\n",
    "num_phone=44\n",
    "max_length=256\n",
    "\n",
    "lr=1e-3\n",
    "weight_decay=5e-7\n",
    "betas=(0.95, 0.999)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "gopt_model = PrepModel(\n",
    "    embed_dim=embed_dim, num_heads=num_heads, \n",
    "    depth=depth, input_dim=input_dim, \n",
    "    max_length=max_length, num_phone=num_phone, dropout=0.1).to(device)\n",
    "\n",
    "trainables = [p for p in gopt_model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    trainables, lr, \n",
    "    weight_decay=weight_decay, \n",
    "    betas=betas\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_losses(\n",
    "        phone_preds, phone_labels, word_preds, word_labels, \n",
    "        utterance_preds, utterance_labels, fluency_preds, fluency_labels,\n",
    "        intonation_preds, intonation_labels):\n",
    "    \n",
    "    # phone level\n",
    "    mask = phone_labels >=0\n",
    "    phone_preds = phone_preds.squeeze(2) * mask\n",
    "    phone_labels = phone_labels * mask\n",
    "    \n",
    "    loss_phn = loss_fn(phone_preds, phone_labels)\n",
    "    loss_phn = loss_phn * (mask.shape[0] * mask.shape[1]) / torch.sum(mask)\n",
    "\n",
    "    # utterance level\n",
    "    loss_utt = loss_fn(utterance_preds.squeeze(1) ,utterance_labels)\n",
    "\n",
    "    loss_utt_flu = loss_fn(fluency_preds.squeeze(1) ,fluency_labels)\n",
    "    loss_utt_int = loss_fn(intonation_preds.squeeze(1) ,intonation_labels)\n",
    "\n",
    "    # word level\n",
    "    mask = word_labels >= 0      \n",
    "    word_preds = word_preds.squeeze(2) * mask\n",
    "    word_labels = word_labels * mask\n",
    "    \n",
    "    loss_word = loss_fn(word_preds, word_labels)\n",
    "    loss_word = loss_word * (mask.shape[0] * mask.shape[1]) / torch.sum(mask)\n",
    "\n",
    "    return loss_phn, loss_utt, loss_word, loss_utt_flu, loss_utt_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "best_mse = 1e5\n",
    "num_epoch = 20\n",
    "phone_weight = 1.0\n",
    "word_weight = 1.0\n",
    "utterance_weight = 1.0\n",
    "\n",
    "cur_lr = lr\n",
    "for epoch in range(num_epoch):\n",
    "    if epoch >= 5 and epoch % 3 == 0:\n",
    "        cur_lr = (4 / 5) * cur_lr \n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = cur_lr\n",
    "\n",
    "    gopt_model.train()\n",
    "    train_tqdm = tqdm(dataloader, \"Training\")\n",
    "    for batch in train_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ids, features, phone_ids, word_ids, relative_positions, \\\n",
    "            phone_labels, word_labels, utterance_labels, \\\n",
    "            fluency_labels, intonation_labels = to_device(batch, device)\n",
    "        \n",
    "        utterance_preds, phone_preds, word_preds, flu_preds, int_preds = gopt_model(\n",
    "            x=features.float(), phn=phone_ids.long(), rel_pos=relative_positions.long())\n",
    "                \n",
    "        loss_phn, loss_utt, loss_word, loss_utt_flu, loss_utt_int = calculate_losses(\n",
    "            phone_preds=phone_preds, \n",
    "            phone_labels=phone_labels, \n",
    "            word_preds=word_preds, \n",
    "            word_labels=word_labels, \n",
    "            utterance_preds=utterance_preds, \n",
    "            utterance_labels=utterance_labels,\n",
    "            fluency_preds=flu_preds, \n",
    "            fluency_labels=fluency_labels,\n",
    "            intonation_preds=int_preds,\n",
    "            intonation_labels=intonation_labels\n",
    "        )\n",
    "\n",
    "        loss = phone_weight*loss_phn + word_weight*loss_word + \\\n",
    "            utterance_weight*(loss_utt + loss_utt_flu + loss_utt_int)/3\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gopt_model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        global_step += 1\n",
    "        train_tqdm.set_postfix(\n",
    "            lr=cur_lr,\n",
    "            loss=loss.item(), \n",
    "            loss_phn=loss_phn.item(), \n",
    "            loss_word=loss_word.item(), \n",
    "            loss_utt=loss_utt.item())\n",
    "    \n",
    "    valid_result = validate(\n",
    "        epoch=epoch, \n",
    "        optimizer=optimizer,\n",
    "        gopt_model=gopt_model, \n",
    "        testloader=dataloader, \n",
    "        best_mse=best_mse, \n",
    "        ckpt_dir=ckpt_dir,\n",
    "        device=device)\n",
    "    \n",
    "    best_mse = valid_result[\"best_mse\"]\n",
    "    global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "from src.utils.train import (\n",
    "    to_device,\n",
    "    load_data,\n",
    "    load_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, phone_ids_path, word_ids_path, \\\n",
    "    phone_scores_path, word_scores_path, sentence_scores_path, fluency_score_path, intonation_score_path, \\\n",
    "    durations_path, gops_path, relative_positions_path, wavlm_features_path = load_data(in_dir)\n",
    "\n",
    "dataset = PrepDataset(\n",
    "    ids=ids, \n",
    "    phone_ids_path=phone_ids_path, \n",
    "    word_ids_path=word_ids_path, \n",
    "    phone_scores_path=phone_scores_path, \n",
    "    word_scores_path=word_scores_path, \n",
    "    sentence_scores_path=sentence_scores_path, \n",
    "    fluency_score_path=fluency_score_path,\n",
    "    intonation_scores_path=intonation_score_path,\n",
    "    durations_path=durations_path, \n",
    "    gops_path=gops_path, \n",
    "    relative_positions_path=relative_positions_path, \n",
    "    wavlm_features_path=wavlm_features_path,\n",
    "    relative2id_path=relative2id_path, \n",
    "    phone2id_path=phone2id_path,\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=1, \n",
    "    num_workers=1,\n",
    "    shuffle=True, \n",
    "    drop_last=False, \n",
    "    pin_memory=True, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim=32\n",
    "# num_heads=1\n",
    "# depth=3\n",
    "# input_dim=855\n",
    "# num_phone=44\n",
    "# max_length=128\n",
    "\n",
    "# lr=1e-3\n",
    "# weight_decay=5e-7\n",
    "# betas=(0.95, 0.999)\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# gopt_model = PrepModel(\n",
    "#     embed_dim=embed_dim, num_heads=num_heads, \n",
    "#     depth=depth, input_dim=input_dim, \n",
    "#     max_length=max_length, num_phone=num_phone, dropout=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = \"/data/codes/apa/train/exp/ckpts/dev/ckpts-eph=24-mse=0.17229999601840973/model.pt\"\n",
    "# state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "# gopt_model.eval()\n",
    "# gopt_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = []\n",
    "prep_scores = []\n",
    "elsa_scores = []\n",
    "\n",
    "gopt_model.eval()\n",
    "for batch in tqdm(dataloader):\n",
    "    batch_ids, features, phone_ids, word_ids, relative_positions, \\\n",
    "            phone_labels, word_labels, utterance_labels, \\\n",
    "            fluency_labels, intonation_labels = to_device(batch, device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        phone_score = gopt_model.forwar_phn(\n",
    "            x=features.float(), phn=phone_ids.long())\n",
    "        \n",
    "        phone_score = phone_score.squeeze(-1)\n",
    "        \n",
    "    assert phone_score.shape[0] == 1\n",
    "    \n",
    "    phone_ids = [f'{batch_ids[0]}_{index}' for index in range((phone_labels!=-1).sum())]\n",
    "    sample_ids += phone_ids\n",
    "\n",
    "    elsa_scores.append(phone_labels[phone_labels!=-1])\n",
    "    prep_scores.append(phone_score[phone_labels!=-1])\n",
    "\n",
    "elsa_scores = torch.concat(elsa_scores).cpu()\n",
    "prep_scores = torch.concat(prep_scores).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{in_dir}/id', \"w\") as f:\n",
    "    content = \"\\n\".join(sample_ids)\n",
    "    f.write(content)\n",
    "\n",
    "np.save(f'{in_dir}/infer-prep_scores.npy', prep_scores.numpy())\n",
    "np.save(f'{in_dir}/infer-elsa_scores.npy', elsa_scores.numpy())\n",
    "np.save(f'{in_dir}/infer-phone_ids.npy', sample_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/apa/train\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.utils.train import (\n",
    "    convert_score_to_color\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_scores = np.load(f'{in_dir}/infer-prep_scores.npy')\n",
    "elsa_scores = np.load(f'{in_dir}/infer-elsa_scores.npy')\n",
    "ids = np.load(f'{in_dir}/infer-phone_ids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_decisions = convert_score_to_color(torch.from_numpy(prep_scores).clone())\n",
    "prep_decisions = prep_decisions.int().numpy()\n",
    "\n",
    "elsa_decisions = convert_score_to_color(torch.from_numpy(elsa_scores).clone())\n",
    "elsa_decisions = elsa_decisions.int().numpy()\n",
    "\n",
    "print(classification_report(y_true=elsa_decisions, y_pred=prep_decisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0:\"GREEN\", 1:\"YELLOW\", 2:\"RED\"}\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": ids.tolist(),\n",
    "        \"prep\": prep_decisions.tolist(),\n",
    "        \"elsa\": elsa_decisions.tolist(),\n",
    "        \"prep_score\": prep_scores.tolist(),\n",
    "        \"elsa_score\": elsa_scores.tolist(),\n",
    "    }\n",
    ")\n",
    "\n",
    "df[\"prep\"] = df.prep.apply(lambda x: id2label[x])\n",
    "df[\"elsa\"] = df.elsa.apply(lambda x: id2label[x])\n",
    "df[\"uid\"] = df.id.apply(lambda x: x.split(\"_\")[0])\n",
    "df[\"diff\"] = np.abs(df[\"prep_score\"] - df[\"elsa_score\"])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.uid.nunique())\n",
    "print((df[df[\"diff\"] > 30/50].uid.value_counts() > 1).sum())\n",
    "print((df[df[\"diff\"] > 30/50].uid.value_counts() / df.uid.value_counts() > 0.1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore_samples = df[df[\"diff\"] > 30/50].uid.value_counts() > 1\n",
    "ignore_samples = df[df[\"diff\"] > 30/50].uid.value_counts() / df.uid.value_counts() > 0.1\n",
    "ignore_samples = ignore_samples[ignore_samples==True]\n",
    "\n",
    "filtered_samples = df[~df.uid.isin(ignore_samples.index)]\n",
    "print(df.shape)\n",
    "print(filtered_samples.shape)\n",
    "print(filtered_samples.uid.unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_path = f\"{out_dir}/id\"\n",
    "with open(id_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(filtered_samples.uid.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
