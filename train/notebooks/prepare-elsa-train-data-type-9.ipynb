{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/apa/train/\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "from pandarallel import pandarallel\n",
    "import random\n",
    "import re\n",
    "\n",
    "pandarallel.initialize(nb_workers=8, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = \"/data/audio_data/pronunciation_scoring_result/marking_data/9\"\n",
    "audio_dir = \"/data/audio_data/prep_submission_audio/9\"\n",
    "\n",
    "in_csv_path = \"/data/audio_data/pronunciation_scoring_result/merged_info/info_question_type-9_19092023_21122023.csv\"\n",
    "out_jsonl_path = \"data/metadata/raw-jsonl/train-data-type-9.jsonl\"\n",
    "metadata = pd.read_csv(in_csv_path)\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def is_valid_audio(audio_id):\n",
    "    abs_path = os.path.join(audio_dir, f'{audio_id}.wav')\n",
    "    if not os.path.exists(abs_path):\n",
    "        return False\n",
    "    try:\n",
    "        wav, sr = torchaudio.load(abs_path)\n",
    "        if sr != 16000:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "is_exist =  metadata.id.parallel_apply(is_valid_audio)\n",
    "print(metadata.shape)\n",
    "metadata = metadata[is_exist]\n",
    "print(metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(path):\n",
    "    vocab = pd.read_csv(path, sep=\"\\t\", names=[\"word\", \"arpa\"])\n",
    "    lexicon = {}\n",
    "    for name, group in vocab.groupby(\"word\"):\n",
    "        lexicon[name] = set(group[\"arpa\"].tolist())\n",
    "\n",
    "    return lexicon\n",
    "\n",
    "lexicon_path = \"/data/codes/apa/train/exp/dicts/lexicon\"\n",
    "lexicon = load_lexicon(lexicon_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision2color = {\n",
    "    \"correct\": 2,\n",
    "    \"warning\":1,\n",
    "    \"incorrect\":0\n",
    "}\n",
    "\n",
    "def norm_text(text):\n",
    "    text = re.sub(r\"[\\,\\.\\!\\?\\:\\;]\", \" \", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "    text = text.upper()\n",
    "\n",
    "    return text\n",
    "\n",
    "def is_valid_phoneme(phoneme):\n",
    "    if phoneme[\"phoneme_error_arpabet\"] != \"normal\":\n",
    "        trans = phoneme[\"phoneme_error_arpabet\"].split(\" - \")[-1]\n",
    "        labels = phoneme[\"phoneme_error_arpabet\"].split(\" - \")[0]\n",
    "        if len(labels.split(\" \")) >= 2:\n",
    "            return False\n",
    "        \n",
    "        if len(trans.split(\" \")) >= 2:\n",
    "            return False\n",
    "        \n",
    "        # if labels in trans:\n",
    "        #     return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def is_valid_word(word):\n",
    "    if norm_text(word[\"text\"]) not in lexicon:\n",
    "        return False\n",
    "\n",
    "    # if word[\"trans_arpabet\"] not in lexicon[norm_text(word[\"text\"])]:\n",
    "    #     return False\n",
    "\n",
    "    if len(word[\"phonemes\"]) != len(word[\"trans_arpabet\"].split()):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "            \n",
    "def parse_metadata_data(json_path):\n",
    "    try: \n",
    "        with open(json_path, \"r\") as f:\n",
    "            content = json.load(f)\n",
    "        id = os.path.basename(json_path).split(\".\")[0]\n",
    "\n",
    "        utterances = []\n",
    "\n",
    "        for raw_utterance in content[\"utterances\"]:\n",
    "            id = id\n",
    "            utt_raw_text = raw_utterance[\"text\"]\n",
    "            utt_uid = raw_utterance[\"utterance_id\"]\n",
    "            start_time = raw_utterance[\"start_time\"]\n",
    "            end_time = raw_utterance[\"end_time\"]\n",
    "            \n",
    "            raw_utterance = raw_utterance[\"result\"]\n",
    "            \n",
    "            utt_score = raw_utterance[\"nativeness_score\"]\n",
    "\n",
    "            utt_text = []\n",
    "            utt_arpas = []\n",
    "            utt_trans = [] \n",
    "            utt_phone_scores = []\n",
    "            utt_decisions = []\n",
    "            utt_word_scores = []\n",
    "            utt_word_ids = []\n",
    "            \n",
    "            ignore = False\n",
    "            for word_id, word in enumerate(raw_utterance[\"words\"]):\n",
    "                word[\"trans_arpabet\"] = word[\"trans_arpabet\"].replace(\"AH0\", \"AX\")\n",
    "                \n",
    "                if is_valid_word(word) == False:\n",
    "                    ignore = True\n",
    "                    break\n",
    "\n",
    "                for phoneme in word[\"phonemes\"]:\n",
    "                    if is_valid_phoneme(phoneme) == False:\n",
    "                        ignore = True\n",
    "                        break\n",
    "\n",
    "                    arpa = phoneme[\"trans_arpabet\"]\n",
    "                    decision = decision2color[phoneme[\"decision\"]]\n",
    "                    score = phoneme[\"nativeness_score\"] if phoneme[\"nativeness_score\"] >= 0 else 0\n",
    "                    tran = phoneme[\"trans_arpabet\"]\n",
    "\n",
    "                    utt_phone_scores.append(score)\n",
    "                    utt_word_ids.append(word_id)\n",
    "                    utt_trans.append(tran)\n",
    "                    utt_decisions.append(decision)\n",
    "\n",
    "                wrd_score = word[\"nativeness_score\"]\n",
    "                wrd_text = norm_text(word[\"text\"])\n",
    "                wrd_arpa = word[\"trans_arpabet\"].split()\n",
    "                \n",
    "                utt_text.append(wrd_text)                \n",
    "                utt_word_scores.append(wrd_score)\n",
    "                utt_arpas += wrd_arpa\n",
    "            \n",
    "\n",
    "            utterance = {\n",
    "                \"id\": id,\n",
    "                \"raw_text\": utt_raw_text,\n",
    "                \"text\": \" \".join(utt_text),\n",
    "                \"utt_id\": utt_uid,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"arpas\": utt_arpas,\n",
    "                \"trans\": utt_trans,\n",
    "                \"phone_scores\": utt_phone_scores,\n",
    "                \"word_scores\": utt_word_scores,\n",
    "                \"decisions\": utt_decisions,\n",
    "                \"word_ids\": utt_word_ids,\n",
    "                \"utterance_score\": utt_score,\n",
    "            }\n",
    "            \n",
    "            if ignore == False:\n",
    "                utterances.append(utterance)\n",
    "        \n",
    "        return utterances\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "extracted_data = metadata.id.parallel_apply(lambda x: parse_metadata_data(os.path.join(json_dir, f'{x}.json')))\n",
    "extracted_data.head()\n",
    "# metadata.id.head(9).apply(lambda x: parse_metadata_data(os.path.join(json_dir, f'{x}.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extracted_data.explode().reset_index()[\"id\"]\n",
    "data = pd.DataFrame({\"data\": data})\n",
    "print(data.shape)\n",
    "data.dropna(inplace=True)\n",
    "data[\"text\"] = data[\"data\"].apply(lambda x: x[\"text\"])\n",
    "data = data.reset_index()[[\"data\"]]\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = data[\"data\"].apply(lambda x: x[\"utterance_score\"]).to_list()\n",
    "\n",
    "pd.DataFrame(scores, columns=[\"score\"]).score.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = data[\"data\"].apply(lambda x: x[\"word_scores\"]).to_list()\n",
    "scores = [score for sample in scores for score in sample]\n",
    "\n",
    "pd.DataFrame(scores, columns=[\"score\"]).score.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = data[\"data\"].apply(lambda x: x[\"phone_scores\"]).to_list()\n",
    "scores = [score for sample in scores for score in sample]\n",
    "\n",
    "scores = pd.DataFrame(scores, columns=[\"score\"])\n",
    "print(scores[scores.score<80].shape)\n",
    "scores.score.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(text):\n",
    "    text = norm_text(text)\n",
    "    if \" AH \" in text:\n",
    "        return False\n",
    "    if \" UH \" in text:\n",
    "        return False\n",
    "    if \" UHM \" in text:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "is_valid = data[\"data\"].apply(lambda x: filter_text(x[\"raw_text\"]))\n",
    "\n",
    "print(data[is_valid].shape)\n",
    "print(data[~is_valid].shape)\n",
    "\n",
    "data = data[is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def copy_audio(row):\n",
    "    min_duration = 3.0\n",
    "\n",
    "    in_audio_dir = \"/data/audio_data/prep_submission_audio/9\"\n",
    "    out_audio_dir = \"/data/codes/apa/train/data/metadata/wav\"\n",
    "\n",
    "    id = row[\"id\"]\n",
    "    utt_id = row[\"utt_id\"]\n",
    "\n",
    "    in_path = f'{in_audio_dir}/{id}.wav'\n",
    "    out_path = f'{out_audio_dir}/{id}-{utt_id}.wav'\n",
    "\n",
    "    start_time = int(row[\"start_time\"] * 16000)\n",
    "    end_time = int(row[\"end_time\"] * 16000)\n",
    "\n",
    "    wav, sr = librosa.load(in_path, sr=16000)\n",
    "\n",
    "    if end_time > wav.shape[0]:\n",
    "        return False\n",
    "    \n",
    "    if wav[start_time:end_time].shape[0] / sr < min_duration:\n",
    "        return False\n",
    "    \n",
    "    assert \"audio_data\" not in out_path\n",
    "\n",
    "    # sf.write(out_path, data=wav[start_time:end_time], samplerate=sr)\n",
    "\n",
    "    return True\n",
    "\n",
    "is_valid = data[\"data\"].parallel_apply(copy_audio)\n",
    "\n",
    "print(data[is_valid].shape)\n",
    "print(data[~is_valid].shape)\n",
    "\n",
    "data = data[is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def update_id(row):\n",
    "    id = row[\"id\"]\n",
    "    utt_id = row[\"utt_id\"]\n",
    "\n",
    "    row[\"id\"] = f'{id}-{utt_id}'\n",
    "\n",
    "    return row\n",
    "\n",
    "data[\"data\"] = data[\"data\"].parallel_apply(update_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = data[\"data\"].apply(lambda x: len(x[\"arpas\"]))\n",
    "print(data.shape)\n",
    "data = data[length < 64]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    " \n",
    "extracted_data = data\n",
    "with open(out_jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for index in tqdm(extracted_data.index):\n",
    "        sample = extracted_data.loc[index, \"data\"]\n",
    "        json_obj = json.dumps(sample)\n",
    "\n",
    "        f.write(f'{json_obj}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
