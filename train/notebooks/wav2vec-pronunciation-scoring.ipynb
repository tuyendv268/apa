{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Wav2Vec2Processor, Wav2Vec2Model\n",
    ")\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import json\n",
    "import pandas as pd \n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alignment(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        lines = [json.loads(line.strip()) for line in lines]\n",
    "\n",
    "    return lines\n",
    "\n",
    "def load_id(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        lines = [line.strip() for line in lines]\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    phone_ids = np.load(f'{data_dir}/phone_ids.npy')\n",
    "    \n",
    "    phone_scores = np.load(f'{data_dir}/phone_scores.npy')\n",
    "    sentence_scores = np.load(f'{data_dir}/sentence_scores.npy')\n",
    "\n",
    "    ids = load_id(f'{data_dir}/id')\n",
    "    gops = np.load(f'{data_dir}/gop.npy')\n",
    "    alignments = load_alignment(f'{data_dir}/alignment')\n",
    "    return ids, phone_ids, phone_scores, sentence_scores, gops, alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader\n",
    ")\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def pad_1d(inputs, max_length=None, pad_value=0.0):\n",
    "    if max_length is None:\n",
    "        lengths = [len(sample) for sample in inputs]\n",
    "        max_length = max(lengths)\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        if inputs[i].shape[0] < max_length:\n",
    "            inputs[i] = torch.cat(\n",
    "                (\n",
    "                    inputs[i], \n",
    "                    pad_value * torch.ones(max_length-inputs[i].shape[0])),\n",
    "                dim=0\n",
    "            )\n",
    "        else:\n",
    "            inputs[i] = inputs[i][0:max_length]\n",
    "    inputs = torch.stack(inputs, dim=0)\n",
    "    return inputs\n",
    "\n",
    "class Wav2vec2Dataset(Dataset):\n",
    "    def __init__(self, audio_dir, ids, phone_ids, phone_scores, sentence_scores, gops, alignments, processor):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.ids = ids\n",
    "        self.phone_ids = phone_ids\n",
    "        self.phone_scores = phone_scores\n",
    "        self.sentence_scores = sentence_scores\n",
    "        self.gops = gops\n",
    "        self.alignments = alignments\n",
    "\n",
    "        self.processor = processor\n",
    "        self.max_utt_length = 32\n",
    "\n",
    "    def load_audio(self, path):\n",
    "        wav, sr = librosa.load(path, sr=16000)\n",
    "\n",
    "        return wav\n",
    "    \n",
    "    def parse_data(self, alignment, id, phone_id, phone_score, sentence_score, gop):\n",
    "        path = f'{self.audio_dir}/{id}.wav'\n",
    "        audio = self.load_audio(path)\n",
    "        \n",
    "        align_indices = self.get_indices_from_aligments(\n",
    "            alignment=alignment\n",
    "        )\n",
    "\n",
    "        audio = self.processor(\n",
    "            audio, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
    "        \n",
    "        assert audio.shape[0] == 1\n",
    "        audio = audio.squeeze(0)\n",
    "\n",
    "        phone_id = torch.tensor(phone_id)\n",
    "        phone_score = torch.tensor(phone_score)\n",
    "        sentence_score = torch.tensor(sentence_score)\n",
    "        gop = torch.tensor(gop)\n",
    "        \n",
    "        phone_score[phone_score != -1] /= 50\n",
    "        sentence_score /= 50\n",
    "\n",
    "        return {\n",
    "            \"audio\": audio,\n",
    "            \"phone_id\": phone_id,\n",
    "            \"phone_score\": phone_score,\n",
    "            \"sentence_score\": sentence_score,\n",
    "            \"gop\": gop,\n",
    "            \"align_indices\": align_indices\n",
    "        } \n",
    "\n",
    "    def get_indices_from_aligments(self, alignment):\n",
    "        index = 0\n",
    "        indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "        for phoneme, start_frame, duration in alignment:\n",
    "            end_frame = start_frame + duration\n",
    "            indices[start_frame:end_frame] = index\n",
    "            index += 1\n",
    "        \n",
    "        return indices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        phone_id = self.phone_ids[index]\n",
    "        phone_score = self.phone_scores[index]\n",
    "        sentence_score = self.sentence_scores[index]\n",
    "        gop = self.gops[index]\n",
    "        alignment = self.alignments[index]\n",
    "\n",
    "        return self.parse_data(\n",
    "            id=id,\n",
    "            phone_id=phone_id,\n",
    "            phone_score=phone_score,\n",
    "            sentence_score=sentence_score,\n",
    "            gop=gop,\n",
    "            alignment=alignment\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def pad2indices(self, indices):\n",
    "        lengths = [len(sample) for sample in indices]\n",
    "        max_length = max(lengths)\n",
    "\n",
    "        for i in range(len(indices)):\n",
    "            if indices[i].shape[0] < max_length:\n",
    "                padding =  -1 * torch.ones(max_length - len(indices[i]))\n",
    "                indices[i] = torch.cat(\n",
    "                    [\n",
    "                        indices[i],\n",
    "                        padding\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        indices = torch.stack(indices, dim=0)\n",
    "        max_indice = self.max_utt_length - 1\n",
    "        for i, length in enumerate(lengths):\n",
    "            max_current_index = indices[i].max().item()\n",
    "\n",
    "            index = max_current_index\n",
    "            for j in range(0, max_length):\n",
    "                if indices[i][j] != -1:\n",
    "                    continue\n",
    "                \n",
    "                if index < max_indice:\n",
    "                    index += 1\n",
    "                indices[i][j] = index\n",
    "                    \n",
    "        return indices\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        phone_ids = [sample[\"phone_id\"] for sample in batch]\n",
    "        phone_scores = [sample[\"phone_score\"] for sample in batch]\n",
    "        sentence_scores = [sample[\"sentence_score\"] for sample in batch]\n",
    "        gops = [sample[\"gop\"] for sample in batch]\n",
    "        input_values = [sample[\"audio\"] for sample in batch]\n",
    "        align_indices = [sample[\"align_indices\"] for sample in batch]\n",
    "\n",
    "        input_values = pad_1d(input_values, pad_value=0.0)\n",
    "        indices = self.pad2indices(align_indices)\n",
    "\n",
    "        phone_ids = torch.stack(phone_ids, dim=0)\n",
    "        phone_scores = torch.stack(phone_scores, dim=0)\n",
    "        sentence_scores = torch.stack(sentence_scores, dim=0)\n",
    "        gops = torch.stack(gops, dim=0)\n",
    "        \n",
    "        return {\n",
    "            \"indices\": indices,\n",
    "            \"phone_ids\": phone_ids,\n",
    "            \"phone_scores\": phone_scores,\n",
    "            \"sentence_scores\": sentence_scores,\n",
    "            \"gops\": gops,\n",
    "            \"input_values\": input_values\n",
    "        }\n",
    "\n",
    "audio_dir = \"/data/audio_data/prep_submission_audio/10\"\n",
    "data_dir = \"/data/codes/apa/train/exps/features/test/dev\"\n",
    "ids, phone_ids, phone_scores, sentence_scores, gops, alignments = load_data(data_dir)\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "dataset = Wav2vec2Dataset(\n",
    "    audio_dir=audio_dir, \n",
    "    ids=ids, \n",
    "    phone_ids=phone_ids, \n",
    "    phone_scores=phone_scores, \n",
    "    sentence_scores=sentence_scores, \n",
    "    gops=gops, \n",
    "    alignments=alignments, \n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=dataset.collate_fn)\n",
    "for batch in dataloader:\n",
    "    # for i in batch[\"indices\"]:\n",
    "    #     print(i)\n",
    "    # print(batch[\"indices\"][0])\n",
    "    # print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "class PrepModel(Module):\n",
    "    def __init__(self, model):\n",
    "        super(PrepModel, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "        self.num_phone = 43\n",
    "        self.phn_proj = torch.nn.Linear(self.num_phone, 64)\n",
    "        \n",
    "        self.ffw = torch.nn.Linear(\n",
    "            768+64, 768\n",
    "        )\n",
    "        \n",
    "        self.utt_head = torch.nn.Linear(\n",
    "            768, 1\n",
    "        )\n",
    "\n",
    "    def get_phone_level_features(self, features, indices, device=\"cuda:0\"):\n",
    "        feature_indices = torch.arange(features.shape[1]).unsqueeze(-1).to(device)\n",
    "        expanded_indices = feature_indices.expand((-1, 2)).flatten()\n",
    "        features = features[:, expanded_indices]\n",
    "\n",
    "        indices[indices==-1] = indices.max() + 1\n",
    "        indices = torch.nn.functional.one_hot(\n",
    "            indices.long(), num_classes=int(indices.max().item())+1).float().to(device)\n",
    "\n",
    "        # indices = indices / indices.sum(1, keepdim=True)\n",
    "\n",
    "        if indices.shape[1] > features.shape[1]:\n",
    "            features = torch.matmul(\n",
    "                indices[:, 0:features.shape[1]].transpose(1, 2), features)\n",
    "        else:\n",
    "            features = torch.matmul(\n",
    "                indices.transpose(1, 2), features[:, 0:indices.shape[1]])\n",
    "            \n",
    "        return features\n",
    "\n",
    "    def forward(self, input_values, phone_ids, indices, sentence_scores=None, device=\"cuda:0\"):\n",
    "        phn_one_hot = torch.nn.functional.one_hot(\n",
    "            phone_ids.long()+1, num_classes=self.num_phone).float().to(device)\n",
    "        phn_embed = self.phn_proj(phn_one_hot)\n",
    "        \n",
    "        features = self.model.extract_features(input_values)[0]\n",
    "        \n",
    "        features = self.get_phone_level_features(\n",
    "            features=features,\n",
    "            indices=indices\n",
    "        )\n",
    "        \n",
    "        features = torch.cat([features, phn_embed], dim=-1)\n",
    "\n",
    "        features = self.ffw(features)\n",
    "        \n",
    "        features = features.mean(dim=1)\n",
    "        scores = self.utt_head(features)\n",
    "        \n",
    "        if sentence_scores is not None:\n",
    "            loss = self.loss(pred_sentence_scores=scores, label_sentence_scores=sentence_scores)\n",
    "            return loss, scores \n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def loss(self, pred_sentence_scores,label_sentence_scores):\n",
    "        sent_loss = torch.mean((pred_sentence_scores-label_sentence_scores)**2)\n",
    "        \n",
    "        return sent_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/codes/apa/train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /data/codes/apa/train\n",
    "from src.models.wavlm_model import WavLM, WavLMConfig\n",
    "pretrained_path = \"/data/codes/apa/train/exps/ckpts/wavlm-base+.pt\"\n",
    "checkpoint = torch.load(pretrained_path)\n",
    "\n",
    "config = WavLMConfig(checkpoint['cfg'])\n",
    "wav2vec = WavLM(config).cuda()\n",
    "wav2vec.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = \"/data/audio_data/prep_submission_audio/10\"\n",
    "data_dir = \"/data/codes/apa/train/exps/features/test/dev\"\n",
    "ids, phone_ids, phone_scores, sentence_scores, gops, alignments = load_data(data_dir)\n",
    "\n",
    "index = 2500\n",
    "\n",
    "\n",
    "trainset = Wav2vec2Dataset(\n",
    "    audio_dir=audio_dir, \n",
    "    ids=ids[0:2500], \n",
    "    phone_ids=phone_ids[0:2500], \n",
    "    phone_scores=phone_scores[0:2500], \n",
    "    sentence_scores=sentence_scores[0:2500], \n",
    "    gops=gops[0:2500], \n",
    "    alignments=alignments[0:2500], \n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=8, shuffle=True, collate_fn=dataset.collate_fn)\n",
    "\n",
    "testset = Wav2vec2Dataset(\n",
    "    audio_dir=audio_dir, \n",
    "    ids=ids[2500:], \n",
    "    phone_ids=phone_ids[2500:], \n",
    "    phone_scores=phone_scores[2500:], \n",
    "    sentence_scores=sentence_scores[2500:], \n",
    "    gops=gops[2500:], \n",
    "    alignments=alignments[2500:], \n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(testset, batch_size=16, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model = PrepModel(wav2vec).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_utt(predict, target):\n",
    "    utt_mse = np.mean(((predict[:, 0] - target[:, 0]) ** 2).numpy())\n",
    "    utt_mae = np.mean((np.abs(predict[:, 0] - target[:, 0])).numpy())\n",
    "    \n",
    "    utt_corr = np.corrcoef(predict[:, 0], target[:, 0])[0, 1]\n",
    "    return utt_mse, utt_mae, utt_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:38<00:00,  8.09it/s, loss=0.281] \n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.06it/s, loss=0.161] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1725504496183761 0.3362931657852246 0.01324251992383299\n",
      "Train loss: 0.25763531581707955, Test loss: 0.17668090462202188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:38<00:00,  8.15it/s, loss=0.0684]\n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.15it/s, loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16249394062088024 0.33035708415347076 0.06391066009153079\n",
      "Train loss: 0.2906006127503431, Test loss: 0.1689805659613434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:38<00:00,  8.14it/s, loss=0.128] \n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.04it/s, loss=0.145] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1477346029595126 0.31560457250607604 0.027400497056666043\n",
      "Train loss: 0.18689634244439496, Test loss: 0.15267970934217545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:38<00:00,  8.12it/s, loss=0.0693]\n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.11it/s, loss=0.191] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1759101222050824 0.33484905845375523 0.0024265110202011256\n",
      "Train loss: 0.1600074216214537, Test loss: 0.17906056358457048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:38<00:00,  8.09it/s, loss=0.0379]\n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.07it/s, loss=0.183] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15224956957693642 0.31207981221355896 0.10363701217423818\n",
      "Train loss: 0.16314415247525896, Test loss: 0.16635495582128737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:38<00:00,  8.16it/s, loss=0.0323]\n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.17it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16748336000784503 0.3383526340555351 0.04659179328559895\n",
      "Train loss: 0.14695374719063717, Test loss: 0.17738075522980853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:38<00:00,  8.03it/s, loss=0.0844]\n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.05it/s, loss=0.275] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13880625297190777 0.31000552877658066 0.07005133854891882\n",
      "Train loss: 0.14979522637334164, Test loss: 0.1444640910833847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:38<00:00,  8.12it/s, loss=0.0872]\n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.06it/s, loss=0.157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12909420459042448 0.2938289573502949 0.13744630708490985\n",
      "Train loss: 0.1550404574375162, Test loss: 0.1383062030038138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:39<00:00,  8.00it/s, loss=0.166] \n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.12it/s, loss=0.337] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5066941077810867 0.5115063267781924 0.022814030084555802\n",
      "Train loss: 0.20748309306108717, Test loss: 0.5243153287002663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 313/313 [00:38<00:00,  8.11it/s, loss=0.114] \n",
      "Test: 100%|██████████| 124/124 [00:13<00:00,  9.11it/s, loss=0.154] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14550136940647562 0.3142473769847608 0.02143494770187068\n",
      "Train loss: 0.3372367670443356, Test loss: 0.15008283325510927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    train_tqdm = tqdm(train_loader, desc=\"Train\")\n",
    "    for batch in train_tqdm:\n",
    "        indices = batch[\"indices\"].to(device)\n",
    "        input_values = batch[\"input_values\"].to(device)\n",
    "        phone_ids = batch[\"phone_ids\"].to(device)\n",
    "        phone_scores = batch[\"phone_scores\"].to(device)\n",
    "        gops = batch[\"gops\"].to(device)\n",
    "        sentence_scores = batch[\"sentence_scores\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            loss, scores = model(\n",
    "                input_values=input_values,\n",
    "                indices=indices,\n",
    "                sentence_scores=sentence_scores,\n",
    "                phone_ids=phone_ids,\n",
    "                device=device\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        train_tqdm.set_postfix(loss=loss.item())\n",
    "    \n",
    "    pred_scores, label_scores = [], []\n",
    "    val_tqdm = tqdm(val_loader, desc=\"Test\")\n",
    "    for batch in val_tqdm:\n",
    "        indices = batch[\"indices\"].to(device)\n",
    "        input_values = batch[\"input_values\"].to(device)\n",
    "        phone_ids = batch[\"phone_ids\"].to(device)\n",
    "        phone_scores = batch[\"phone_scores\"].to(device)\n",
    "        gops = batch[\"gops\"].to(device)\n",
    "        sentence_scores = batch[\"sentence_scores\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                loss, scores = model(\n",
    "                    input_values=input_values,\n",
    "                    indices=indices,\n",
    "                    sentence_scores=sentence_scores,\n",
    "                    phone_ids=phone_ids,\n",
    "                    device=device\n",
    "                )\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        pred_scores.append(scores)\n",
    "        label_scores.append(sentence_scores.unsqueeze(-1))\n",
    "        \n",
    "        val_losses.append(loss.item())\n",
    "        val_tqdm.set_postfix(loss=loss.item())\n",
    "        \n",
    "    pred_scores = torch.cat(pred_scores, dim=0)\n",
    "    label_scores = torch.cat(label_scores, dim=0)\n",
    "    \n",
    "    utt_mse, utt_mae, utt_corr = valid_utt(\n",
    "        predict=pred_scores.cpu(), \n",
    "        target=label_scores.cpu()\n",
    "    )\n",
    "    \n",
    "    print(utt_mse, utt_mae, utt_corr)\n",
    "        \n",
    "    print(f'Train loss: {np.mean(train_losses)}, Test loss: {np.mean(val_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3776, 1.1086, 0.6452, 1.8580, 1.4176, 1.1550, 1.1400, 1.5274, 1.8448,\n",
       "        0.9392, 1.9248, 1.4496, 1.5958, 1.4460], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phone_level_features(features, indices):\n",
    "    # feature_indices = torch.arange(features.shape[1]).unsqueeze(-1)\n",
    "    # expanded_indices = feature_indices.expand((-1, 2)).flatten()\n",
    "    # features = features[:, expanded_indices]\n",
    "\n",
    "    indices[indices==-1] = indices.max() + 1\n",
    "    indices = torch.nn.functional.one_hot(\n",
    "        indices.long(), num_classes=int(indices.max().item())+1)\n",
    "    indices = indices / indices.sum(1, keepdim=True)\n",
    "\n",
    "    if indices.shape[1] > features.shape[1]:\n",
    "        features = torch.matmul(\n",
    "            indices[:, 0:features.shape[1]].transpose(1, 2), features)\n",
    "    else:\n",
    "        features = torch.matmul(\n",
    "            indices.transpose(1, 2), features[:, 0:indices.shape[1]])\n",
    "        \n",
    "    features = features[:, :-1]\n",
    "    return features\n",
    "\n",
    "def pad2indices(indices, max_utt_length=32):\n",
    "    lengths = [len(sample) for sample in indices]\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        if indices[i].shape[0] < max_length:\n",
    "            padding =  -1 * torch.ones(max_length - len(indices[i]))\n",
    "            indices[i] = torch.cat(\n",
    "                [\n",
    "                    indices[i],\n",
    "                    padding\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    indices = torch.stack(indices, dim=0)\n",
    "    max_indice = max_utt_length - 1\n",
    "    for i, length in enumerate(lengths):\n",
    "        max_current_index = indices[i].max().item()\n",
    "\n",
    "        index = max_current_index\n",
    "        for j in range(length, max_length):\n",
    "            if index < max_indice:\n",
    "                index += 1\n",
    "            indices[i][j] = index\n",
    "                \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "alignment_1 = [(0, 2), (3, 5), (8, 4), (12, 4), (16, 3), (19, 4)]\n",
    "features_1 = -1 * torch.ones(alignment_1[-1][0] + alignment_1[-1][1], 8)\n",
    "indices_1 = -1 * torch.ones(alignment_1[-1][0] + alignment_1[-1][1])\n",
    "for index, (start, duration) in enumerate(alignment_1):\n",
    "    features_1[start:start + duration] = index\n",
    "    indices_1[start:start + duration] = index\n",
    "\n",
    "# get_phone_level_features(features=features_1, indices=indices_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "alignment_2 = [(0, 4), (5, 6), (10, 2), (12, 4)]\n",
    "features_2 = -1 * torch.ones(alignment_2[-1][0] + alignment_2[-1][1], 8)\n",
    "indices_2 = -1 * torch.ones(alignment_2[-1][0] + alignment_2[-1][1])\n",
    "for index, (start, duration) in enumerate(alignment_2):\n",
    "    features_2[start:start + duration] = index\n",
    "    indices_2[start:start + duration] = index\n",
    "\n",
    "# get_phone_level_features(features=features_2, indices=indices_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_2d(inputs, max_length=None, pad_value=0.0):\n",
    "    if max_length is None:\n",
    "        lengths = [len(sample) for sample in inputs]\n",
    "        max_length = max(lengths)\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        if inputs[i].shape[0] < max_length:\n",
    "            inputs[i] = torch.cat(\n",
    "                (\n",
    "                    inputs[i], \n",
    "                    pad_value * torch.ones((max_length-inputs[i].shape[0], inputs[i].shape[1]))),\n",
    "                dim=0\n",
    "            )\n",
    "        else:\n",
    "            inputs[i] = inputs[i][0:max_length]\n",
    "    inputs = torch.stack(inputs, dim=0)\n",
    "    return inputs\n",
    "\n",
    "def pad_1d(inputs, max_length=None, pad_value=0.0):\n",
    "    if max_length is None:\n",
    "        lengths = [len(sample) for sample in inputs]\n",
    "        max_length = max(lengths)\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        if inputs[i].shape[0] < max_length:\n",
    "            inputs[i] = torch.cat(\n",
    "                (\n",
    "                    inputs[i], \n",
    "                    pad_value * torch.ones(max_length-inputs[i].shape[0])),\n",
    "                dim=0\n",
    "            )\n",
    "        else:\n",
    "            inputs[i] = inputs[i][0:max_length]\n",
    "    inputs = torch.stack(inputs, dim=0)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17., 18., 19.,  ..., 31., 31., 31.],\n",
       "        [17., 18., 19.,  ..., 31., 31., 31.],\n",
       "        [17., 18., 19.,  ..., 31., 31., 31.],\n",
       "        ...,\n",
       "        [19., 20., 21.,  ..., 31., 31., 31.],\n",
       "        [15., 16., 17.,  ..., 31., 31., 31.],\n",
       "        [20., 21., 22.,  ..., 31., 31., 31.]], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments = [alignment_1, alignment_2]\n",
    "features = [features_1, features_2]\n",
    "indices = [indices_1, indices_2]\n",
    "\n",
    "features = pad_2d(features)\n",
    "indices = pad2indices(indices, max_utt_length=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0., -1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,  3.,  3.,\n",
       "          3.,  3.,  4.,  4.,  4.,  5.,  5.,  5.,  5.],\n",
       "        [ 0.,  0.,  0.,  0., -1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  3.,  3.,\n",
       "          3.,  3.,  4.,  5.,  5.,  5.,  5.,  5.,  5.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "         [4., 4., 4., 4., 4., 4., 4., 4.],\n",
       "         [5., 5., 5., 5., 5., 5., 5., 5.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "         [3., 3., 3., 3., 3., 3., 3., 3.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_phone_level_features(features, indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
