{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_phone(phones):\n",
    "    mapped_phone = []\n",
    "    for phone in phones:\n",
    "        if phone == \"SCHWA\" or phone == \"AH0\":\n",
    "            mapped_phone.append(\"AX\")\n",
    "        else:\n",
    "            mapped_phone.append(phone)\n",
    "    \n",
    "    return mapped_phone\n",
    "\n",
    "def get_phone_pure(phones):\n",
    "    pure_phones = [re.sub(r\"\\d\", \"\", phone) for phone in phones]\n",
    "\n",
    "    return pure_phones\n",
    "\n",
    "def preprocess_metadata(metadata):\n",
    "    metadata = metadata[\n",
    "        [\"id\", \"audio_path\", \"text\", \"arpas\", \"trans\", \"phone_scores\", \"word_scores\", \"word_ids\", \"utterance_score\"]\n",
    "    ]\n",
    "    metadata[\"id\"] = metadata.id.apply(str)\n",
    "    metadata = metadata.rename(columns={\"arpas\":\"elsa_phone\"})\n",
    "    metadata[\"elsa_phone\"] = metadata.elsa_phone.apply(map_phone)\n",
    "    metadata[\"elsa_phone\"] = metadata.elsa_phone.apply(get_phone_pure)\n",
    "    metadata[\"trans\"] = metadata.trans.apply(map_phone)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [json.loads(line.strip()) for line in lines]\n",
    "    \n",
    "    lines = pd.DataFrame(lines)\n",
    "    return lines\n",
    "\n",
    "def load_gops(gop_paths):\n",
    "    gops = {}\n",
    "    for path in gop_paths:\n",
    "        try:\n",
    "            gop = pickle.load(open(path, \"rb\"))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        for key in gop.keys():\n",
    "            assert key not in gops\n",
    "\n",
    "        gops.update(gop)\n",
    "\n",
    "    return gops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_812143/3694442826.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata[\"id\"] = metadata.id.apply(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40349, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>text</th>\n",
       "      <th>elsa_phone</th>\n",
       "      <th>trans</th>\n",
       "      <th>phone_scores</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>utterance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5796046-1</td>\n",
       "      <td>data/metadata/wav/5796046-1.wav</td>\n",
       "      <td>A A A VERY SOLID GO WHEN I WAS A KID</td>\n",
       "      <td>[AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...</td>\n",
       "      <td>[AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...</td>\n",
       "      <td>[44, 17, 31, 93, 99, 98, 96, 90, 1, 100, 86, 0...</td>\n",
       "      <td>[44, 17, 30, 94, 35, 95, 95, 95, 95, 95, 71]</td>\n",
       "      <td>[0, 1, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 6, ...</td>\n",
       "      <td>70.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                       audio_path  \\\n",
       "0  5796046-1  data/metadata/wav/5796046-1.wav   \n",
       "\n",
       "                                   text  \\\n",
       "0  A A A VERY SOLID GO WHEN I WAS A KID   \n",
       "\n",
       "                                          elsa_phone  \\\n",
       "0  [AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...   \n",
       "\n",
       "                                               trans  \\\n",
       "0  [AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...   \n",
       "\n",
       "                                        phone_scores  \\\n",
       "0  [44, 17, 31, 93, 99, 98, 96, 90, 1, 100, 86, 0...   \n",
       "\n",
       "                                    word_scores  \\\n",
       "0  [44, 17, 30, 94, 35, 95, 95, 95, 95, 95, 71]   \n",
       "\n",
       "                                            word_ids  utterance_score  \n",
       "0  [0, 1, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 6, ...            70.28  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata_path = \"/data/codes/apa/train/prep_data/jsonl/train-data-type-12-v2.jsonl\"\n",
    "# out_dir = \"/data/codes/apa/train/exps/features/train/train-12\"\n",
    "# data_dir = \"/data/codes/apa/train/data/train/train-type-12\"\n",
    "\n",
    "# metadata_path = \"/data/codes/apa/train/prep_data/jsonl/info_out_domain_long_sentence_testset.jsonl\"\n",
    "# out_dir = \"/data/codes/apa/train/exps/features/test/dev\"\n",
    "# data_dir = \"/data/codes/apa/train/data/test/out-long\"\n",
    "\n",
    "# metadata_path = \"/data/codes/apa/train/prep_data/jsonl/info_in_domain_long_sentence_testset.jsonl\"\n",
    "# out_dir = \"/data/codes/apa/train/exps/features/test/info_in_domain_long_sentence_testset\"\n",
    "# data_dir = \"/data/codes/apa/train/data/test/info_in_domain_long_sentence_testset\"\n",
    "\n",
    "# metadata_path = \"/data/codes/apa/train/prep_data/jsonl/info_out_domain_short_sentence_testset.jsonl\"\n",
    "# out_dir = \"/data/codes/apa/train/exps/features/test/out-short\"\n",
    "# data_dir = \"/data/codes/apa/train/data/test/out-short\"\n",
    "\n",
    "metadata_path = \"/data/codes/apa/train/data/metadata/jsonl/train-data-type-9.jsonl\"\n",
    "out_dir = \"/data/codes/apa/train/data/feats/train/train-data-type-9\"\n",
    "data_dir = \"/data/codes/apa/train/data/train/train-data-type-9\"\n",
    "\n",
    "metadata = load_jsonl(metadata_path)\n",
    "metadata = preprocess_metadata(metadata)\n",
    "print(metadata.shape)\n",
    "metadata.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# from pandarallel import pandarallel\n",
    "\n",
    "# pandarallel.initialize(nb_workers=8, progress_bar=True)\n",
    "# out_path = \"/data/codes/apa/train/data/wav\"\n",
    "\n",
    "# def copy_audio(audio_path):\n",
    "#     dst = f'{out_path}/{os.path.basename(audio_path)}'\n",
    "\n",
    "#     shutil.copy(src=audio_path, dst=dst)\n",
    "\n",
    "# metadata.audio_path.parallel_apply(copy_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if out_dir.endswith(\"-filtered\"):\n",
    "    id_path = f\"{out_dir}/id\"\n",
    "\n",
    "    id_df = pd.read_csv(id_path, names=[\"id\"], dtype={'id':str})\n",
    "    id_df = id_df.set_index(\"id\")\n",
    "\n",
    "    print(metadata.shape)\n",
    "    metadata = metadata[metadata.id.isin(id_df.index)]\n",
    "    print(metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "gop_path = f'{data_dir}/*/gop.pkl'\n",
    "align_path = f'{data_dir}/*/ali.out'\n",
    "\n",
    "alignment_paths = glob(align_path)\n",
    "gop_paths = glob(gop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alignment(path):\n",
    "    alignment_df = pd.read_csv(\n",
    "        path, names=[\"id\", \"alignment\"], sep=\"\\t\", dtype={\"id\": str}\n",
    "    )\n",
    "    alignment_df[\"alignment\"] = alignment_df.alignment.apply(json.loads)\n",
    "\n",
    "    return alignment_df\n",
    "\n",
    "def load_alignments(paths):\n",
    "    alignments = []\n",
    "    for path in paths:\n",
    "        alignment = load_alignment(path)\n",
    "        alignments.append(alignment)\n",
    "    \n",
    "    alignments = pd.concat(alignments)\n",
    "    alignments.reset_index(inplace=True)\n",
    "\n",
    "    return alignments[[\"id\", \"alignment\"]]\n",
    "\n",
    "alignments = load_alignments(alignment_paths)\n",
    "gops = load_gops(gop_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40340, 2)\n",
      "(40340, 2)\n"
     ]
    }
   ],
   "source": [
    "is_valid = alignments.id.apply(lambda x: x in gops)\n",
    "print(alignments.shape)\n",
    "alignments = alignments[is_valid]\n",
    "print(alignments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phonemes(alignments):\n",
    "    phonemes = [\n",
    "        re.sub(\"\\d\", \"\",phoneme[0].split(\"_\")[0]) for phoneme in alignments\n",
    "        if phoneme[0] != \"SIL\"\n",
    "    ]\n",
    "    return phonemes\n",
    "\n",
    "def extract_durations(alignments):\n",
    "    durations = [\n",
    "        round(phoneme[2] * 0.02, 4) for phoneme in alignments\n",
    "        if phoneme[0] != \"SIL\"\n",
    "    ]\n",
    "    return durations\n",
    "\n",
    "def extract_relative_positions(alignments):\n",
    "    relative_positions = [\n",
    "        phoneme[0].split(\"_\")[-1] for phoneme in alignments\n",
    "        if phoneme[0] != \"SIL\"\n",
    "    ]\n",
    "    return relative_positions\n",
    "\n",
    "\n",
    "alignments[\"relative_positions\"] = alignments.alignment.apply(lambda x: extract_relative_positions(x))\n",
    "alignments[\"prep_phone\"] = alignments.alignment.apply(lambda x: extract_phonemes(x))\n",
    "alignments[\"duration\"] = alignments.alignment.apply(lambda x: extract_durations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>audio_path</th>\n",
       "      <th>text</th>\n",
       "      <th>elsa_phone</th>\n",
       "      <th>trans</th>\n",
       "      <th>phone_scores</th>\n",
       "      <th>word_scores</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>utterance_score</th>\n",
       "      <th>alignment</th>\n",
       "      <th>prep_phone</th>\n",
       "      <th>relative_positions</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5796046-1</td>\n",
       "      <td>data/metadata/wav/5796046-1.wav</td>\n",
       "      <td>A A A VERY SOLID GO WHEN I WAS A KID</td>\n",
       "      <td>[AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...</td>\n",
       "      <td>[AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...</td>\n",
       "      <td>[44, 17, 31, 93, 99, 98, 96, 90, 1, 100, 86, 0...</td>\n",
       "      <td>[44, 17, 30, 94, 35, 95, 95, 95, 95, 95, 71]</td>\n",
       "      <td>[0, 1, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 6, ...</td>\n",
       "      <td>70.28</td>\n",
       "      <td>[[SIL, 0, 18], [AX_S, 18, 10], [SIL, 28, 46], ...</td>\n",
       "      <td>[AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...</td>\n",
       "      <td>[S, S, S, B, I, I, E, B, I, I, I, E, B, E, B, ...</td>\n",
       "      <td>[0.2, 0.26, 0.2, 0.24, 0.22, 0.22, 0.16, 0.34,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                       audio_path  \\\n",
       "0  5796046-1  data/metadata/wav/5796046-1.wav   \n",
       "\n",
       "                                   text  \\\n",
       "0  A A A VERY SOLID GO WHEN I WAS A KID   \n",
       "\n",
       "                                          elsa_phone  \\\n",
       "0  [AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...   \n",
       "\n",
       "                                               trans  \\\n",
       "0  [AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...   \n",
       "\n",
       "                                        phone_scores  \\\n",
       "0  [44, 17, 31, 93, 99, 98, 96, 90, 1, 100, 86, 0...   \n",
       "\n",
       "                                    word_scores  \\\n",
       "0  [44, 17, 30, 94, 35, 95, 95, 95, 95, 95, 71]   \n",
       "\n",
       "                                            word_ids  utterance_score  \\\n",
       "0  [0, 1, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 6, ...            70.28   \n",
       "\n",
       "                                           alignment  \\\n",
       "0  [[SIL, 0, 18], [AX_S, 18, 10], [SIL, 28, 46], ...   \n",
       "\n",
       "                                          prep_phone  \\\n",
       "0  [AX, AX, AX, V, EH, R, IY, S, AA, L, IH, D, G,...   \n",
       "\n",
       "                                  relative_positions  \\\n",
       "0  [S, S, S, B, I, I, E, B, I, I, I, E, B, E, B, ...   \n",
       "\n",
       "                                            duration  \n",
       "0  [0.2, 0.26, 0.2, 0.24, 0.22, 0.22, 0.16, 0.34,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.merge(\n",
    "    left=metadata, \n",
    "    right=alignments[[\"id\", \"alignment\", \"prep_phone\", \"relative_positions\", \"duration\"]], \n",
    "    how=\"inner\", on=\"id\"\n",
    ")\n",
    "\n",
    "metadata.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_match(elsa, prep, scores):\n",
    "#     for index, (phone_1, phone_2) in enumerate(zip(elsa, prep)):\n",
    "#         if phone_1 != phone_2:\n",
    "#             if scores[index] < 40:\n",
    "#                 continue\n",
    "#             return 0\n",
    "        \n",
    "#     return 1\n",
    "\n",
    "# is_matched = metadata.apply(lambda x: count_match(elsa=x[\"elsa_phone\"], prep=x[\"prep_phone\"], scores=x[\"phone_scores\"]), axis=1)\n",
    "# metadata = metadata[is_matched==True]\n",
    "# print(is_matched.sum())\n",
    "# print(is_matched.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = metadata[\"elsa_phone\"].apply(len)\n",
    "lengths.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40340, 13)\n",
      "(40340, 13)\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 66\n",
    "\n",
    "print(metadata.shape)\n",
    "metadata = metadata[lengths<MAX_LENGTH-1]\n",
    "print(metadata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_alignments(alignment):\n",
    "    processed_alignment = []\n",
    "    for phone, start, duration in alignment:\n",
    "        if phone == \"SIL\":\n",
    "            continue\n",
    "        phone = phone.split(\"_\")[0]\n",
    "        processed_alignment.append([phone, start, duration])\n",
    "    \n",
    "    return processed_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40340/40340 [00:00<00:00, 68525.72it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f'{out_dir}/alignment', \"w\", encoding=\"utf-8\") as f:\n",
    "    for index in tqdm(metadata.index):\n",
    "        _id = metadata[\"id\"][index]\n",
    "        _alignment = metadata[\"alignment\"][index]\n",
    "\n",
    "        _alignment = preprocess_alignments(_alignment)\n",
    "\n",
    "        _alignment= json.dumps(_alignment, ensure_ascii=False)\n",
    "        \n",
    "        f.write(f'{_alignment}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40340/40340 [00:00<00:00, 507554.07it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f'{out_dir}/id', \"w\", encoding=\"utf-8\") as f:\n",
    "    for index in tqdm(metadata.index):\n",
    "        _id = metadata[\"id\"][index]\n",
    "        \n",
    "        f.write(f'{_id}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract gop feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40340/40340 [00:12<00:00, 3278.16it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [65, 86] at entry 0 and [66, 86] at entry 14781",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     gop \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(gop)\n\u001b[1;32m     10\u001b[0m     gop_features\u001b[38;5;241m.\u001b[39mappend(gop)\n\u001b[0;32m---> 12\u001b[0m gop_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgop_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m gop_features \u001b[38;5;241m=\u001b[39m gop_features\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     14\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/gop.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, gop_features)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [65, 86] at entry 0 and [66, 86] at entry 14781"
     ]
    }
   ],
   "source": [
    "gop_features = []\n",
    "\n",
    "for index in tqdm(metadata.index):\n",
    "    _id = metadata[\"id\"][index]\n",
    "    gop = gops[_id]\n",
    "\n",
    "    padding = [[0,]*len(gop[0]),]*(MAX_LENGTH-len(gop))\n",
    "    gop = gop.tolist() + padding\n",
    "    gop = torch.tensor(gop)\n",
    "    gop_features.append(gop)\n",
    "\n",
    "gop_features = torch.stack(gop_features, dim=0)\n",
    "gop_features = gop_features.numpy()\n",
    "np.save(f'{out_dir}/gop.npy', gop_features)\n",
    "gop_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Relative Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/codes/apa/train/exp/dicts/relative2id.json\"\n",
    "relative2id = json.load(open(path, \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relative_position_to_id(relative_positions):\n",
    "    ids = []\n",
    "    for rel_pos in relative_positions:\n",
    "        _id = relative2id[rel_pos]\n",
    "        ids.append(_id)\n",
    "\n",
    "    return ids\n",
    "metadata[\"relative_positions\"] = metadata[\"relative_positions\"].apply(convert_relative_position_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_positions = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    relative_position = metadata[\"relative_positions\"][index].copy()\n",
    "\n",
    "    padding = [relative2id[\"PAD\"],]*(MAX_LENGTH-len(relative_position))\n",
    "    relative_position = relative_position + padding\n",
    "    relative_position = torch.tensor(relative_position)\n",
    "    relative_positions.append(relative_position)\n",
    "\n",
    "relative_positions = torch.stack(relative_positions, dim=0)\n",
    "relative_positions = relative_positions.numpy()\n",
    "np.save(f'{out_dir}/relative_positions.npy', relative_positions)\n",
    "relative_positions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    sentence_score = metadata[\"utterance_score\"][index].copy()\n",
    "\n",
    "    sentence_scores.append(sentence_score)\n",
    "\n",
    "sentence_scores = torch.tensor(sentence_scores)\n",
    "sentence_scores = sentence_scores.numpy()\n",
    "print(sentence_scores.shape)\n",
    "np.save(f'{out_dir}/sentence_scores.npy', sentence_scores)\n",
    "sentence_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    word_score = metadata[\"word_scores\"][index].copy()\n",
    "    word_id = metadata[\"word_ids\"][index].copy()\n",
    "\n",
    "    word_score_in_phone_levels = []\n",
    "    for wid in word_id:\n",
    "        word_score_in_phone_levels.append(word_score[wid])\n",
    "\n",
    "    padding = [-1,]*(MAX_LENGTH-len(word_score_in_phone_levels))\n",
    "    word_score_in_phone_levels = word_score_in_phone_levels + padding\n",
    "    word_score_in_phone_levels = torch.tensor(word_score_in_phone_levels)\n",
    "    word_scores.append(word_score_in_phone_levels)\n",
    "\n",
    "word_scores = torch.stack(word_scores, dim=0)\n",
    "word_scores = word_scores.numpy()\n",
    "print(word_scores.shape)\n",
    "np.save(f'{out_dir}/word_scores.npy', word_scores)\n",
    "word_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    word_id = metadata[\"word_ids\"][index].copy()\n",
    "\n",
    "    padding = [-1,]*(MAX_LENGTH-len(word_id))\n",
    "    word_id = word_id + padding\n",
    "    word_id = torch.tensor(word_id)\n",
    "    word_ids.append(word_id)\n",
    "\n",
    "word_ids = torch.stack(word_ids, dim=0)\n",
    "word_ids = word_ids.numpy()\n",
    "print(word_ids.shape)\n",
    "np.save(f'{out_dir}/word_ids.npy', word_ids)\n",
    "word_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract duration feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    duration = metadata[\"duration\"][index].copy()\n",
    "\n",
    "    padding = [0, ]*(MAX_LENGTH-len(duration))\n",
    "\n",
    "    duration += padding\n",
    "    duration = torch.tensor(duration)\n",
    "    durations.append(duration)\n",
    "\n",
    "durations = torch.stack(durations, dim=0)\n",
    "durations = durations.numpy()\n",
    "np.save(f'{out_dir}/duration.npy', durations)\n",
    "durations = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract phone scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_scores = []\n",
    "\n",
    "for index in metadata.index:\n",
    "    phone_score = metadata[\"phone_scores\"][index].copy()\n",
    "\n",
    "    padding = [-1, ]*(MAX_LENGTH-len(phone_score))\n",
    "\n",
    "    phone_score += padding\n",
    "    phone_score = torch.tensor(phone_score)\n",
    "    phone_scores.append(phone_score)\n",
    "\n",
    "phone_scores = torch.stack(phone_scores, dim=0)\n",
    "phone_scores = phone_scores.numpy()\n",
    "np.save(f'{out_dir}/phone_scores.npy', phone_scores)\n",
    "phone_scores = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract phone ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_dict_path =  \"/data/codes/apa/train/exp/dicts/phone_dict.json\"\n",
    "with open(phone_dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    phone_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_ids = []\n",
    "\n",
    "pad_token_id = phone_dict[\"PAD\"]\n",
    "for index in metadata.index:\n",
    "    phoneme = metadata[\"elsa_phone\"][index].copy()\n",
    "\n",
    "    phoneme = [re.sub(\"\\d\", \"\", phn) for phn in phoneme]\n",
    "    phoneme = [phone_dict[phn] for phn in phoneme]\n",
    "    padding = [pad_token_id, ]*(MAX_LENGTH-len(phoneme))\n",
    "\n",
    "    phoneme += padding\n",
    "    phone_ids.append(torch.tensor(phoneme))\n",
    "\n",
    "phone_ids = torch.stack(phone_ids, dim=0)\n",
    "phone_ids = phone_ids.numpy()\n",
    "np.save(f'{out_dir}/phone_ids.npy', phone_ids)\n",
    "phone_ids = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract WavLM Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/apa/train\n",
    "import torch\n",
    "from src.models.wavlm_model import WavLM, WavLMConfig\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = \"/data/codes/apa/train/exp/torch/wavlm-base+.pt\"\n",
    "checkpoint = torch.load(pretrained_path)\n",
    "\n",
    "config = WavLMConfig(checkpoint['cfg'])\n",
    "model = WavLM(config).eval().cuda()\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(alignment, features):\n",
    "    index = 0\n",
    "    phonemes = []\n",
    "    indices = -1 * torch.ones(alignment[-1][1] + alignment[-1][2])\n",
    "    for phoneme, start_frame, duration in alignment:\n",
    "        if phoneme == \"SIL\":\n",
    "            continue\n",
    "        end_frame = start_frame + duration\n",
    "        indices[start_frame:end_frame] = index\n",
    "        phonemes.append(phoneme)\n",
    "        index += 1\n",
    "\n",
    "    if -1 in indices:\n",
    "        indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "        indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "        indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "        if features.shape[0] != indices.shape[0]:\n",
    "            features = features[0:indices.shape[0]]\n",
    "        features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "        return features[:-1].cpu(), phonemes\n",
    "    \n",
    "    else:\n",
    "        indices[indices==-1] = indices.max() + 1\n",
    "\n",
    "        indices = torch.nn.functional.one_hot(indices.long(), num_classes=int(indices.max().item())+1).cuda()\n",
    "        indices = indices / indices.sum(0, keepdim=True)\n",
    "    \n",
    "        if features.shape[0] != indices.shape[0]:\n",
    "            features = features[0:indices.shape[0]]\n",
    "        features = torch.matmul(indices.transpose(0, 1), features)\n",
    "\n",
    "        return features.cpu(), phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavlm_features = []\n",
    "for index in tqdm(metadata.index):\n",
    "    audio_path = metadata[\"audio_path\"][index]\n",
    "    alignment = metadata[\"alignment\"][index]\n",
    "\n",
    "    wav, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    input_values = torch.from_numpy(wav).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        features = model.extract_features(input_values)[0]\n",
    "        if index % 1000:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    index = torch.arange(features.shape[1]).unsqueeze(-1)\n",
    "    expanded_index = index.expand((-1, 2)).flatten()\n",
    "    features = features[0][expanded_index]\n",
    "\n",
    "    features, phonemes = extract_feature(alignment, features)\n",
    "    if len(features) != len(phonemes):\n",
    "        print(metadata[\"id\"][index])\n",
    "\n",
    "    features = torch.concat([features, torch.zeros(MAX_LENGTH-len(features), 768)], axis=0)\n",
    "    wavlm_features.append(features.unsqueeze(0).numpy())\n",
    "\n",
    "wavlm_features = np.row_stack(wavlm_features)\n",
    "np.save(f'{out_dir}/wavlm_features.npy', wavlm_features)\n",
    "wavlm_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build index dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /data/codes/apa/train/\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from src.dataset import (\n",
    "    IndexedDataset,\n",
    "    IndexedDatasetBuilder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_path = f\"{out_dir}/wavlm_features.npy\"\n",
    "indexed_path = f'{out_dir}/wavlm_features'\n",
    "\n",
    "data = np.load(npy_path)\n",
    "builder = IndexedDatasetBuilder(indexed_path)\n",
    "\n",
    "for index in tqdm(range(data.shape[0])):\n",
    "    builder.add_item(item=data[index])\n",
    "\n",
    "builder.finalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
